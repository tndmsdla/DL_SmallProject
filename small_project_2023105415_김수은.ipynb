{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# 훈련셋, 테스트셋 준비\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainSet = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "trainLoader = torch.utils.data.DataLoader(\n",
        "    trainSet, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testSet = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testLoader = torch.utils.data.DataLoader(testSet, batch_size=100,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# BasicBlock 구현\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inputCh, outputCh, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.firstConv = nn.Conv2d(inputCh, outputCh, kernel_size=3,\n",
        "                                   stride=stride, padding=1)\n",
        "        self.firstBN = nn.BatchNorm2d(outputCh)\n",
        "\n",
        "        self.secondConv = nn.Conv2d(outputCh, outputCh, kernel_size=3,\n",
        "                                    stride=1, padding=1)\n",
        "        self.secondBN = nn.BatchNorm2d(outputCh)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or inputCh != outputCh:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(inputCh, outputCh, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(outputCh))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.firstBN(self.firstConv(x)))\n",
        "        out = self.secondBN(self.secondConv(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "# ResNet20 구현\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.inputCh = 16\n",
        "\n",
        "        self.conv = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.BN = nn.BatchNorm2d(16)\n",
        "\n",
        "        self.firstLayer = self.makeLayer(block, 16, num_blocks[0], stride=1)\n",
        "        self.secondLayer = self.makeLayer(block, 32, num_blocks[1], stride=2)\n",
        "        self.thirdLayer = self.makeLayer(block, 64, num_blocks[2], stride=2)\n",
        "\n",
        "        self.averagePooling = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fullyConnectedLayer = nn.Linear(64 * block.expansion, num_classes)\n",
        "\n",
        "    def makeLayer(self, block, outputCh, blocks, stride):\n",
        "        strides = [stride] + [1] * (blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.inputCh, outputCh, stride))\n",
        "            self.inputCh = outputCh * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.BN(self.conv(x)))\n",
        "        out = self.firstLayer(out)\n",
        "        out = self.secondLayer(out)\n",
        "        out = self.thirdLayer(out)\n",
        "        out = self.averagePooling(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fullyConnectedLayer(out)\n",
        "        return out\n",
        "\n",
        "def ResNet20():\n",
        "    return ResNet(BasicBlock, [3, 3, 3])\n",
        "\n",
        "net = ResNet20()\n",
        "\n",
        "# 필요 시 GPU로 이동\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "net = net.to(device)\n",
        "\n",
        "# 손실 함수 및 옵티마이저 설정\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
        "\n",
        "# 훈련 및 테스트 시작\n",
        "def train(epoch):\n",
        "    net.train()\n",
        "    trainLoss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batchIdx, (inputs, targets) in enumerate(trainLoader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        trainLoss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if batchIdx % 100 == 0:\n",
        "            print(f'Epoch: {epoch} | Batch: {batchIdx} | Loss: {trainLoss/(batchIdx+1)} | Acc: {100.*correct/total}')\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    net.eval()\n",
        "    testLoss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batchIdx, (inputs, targets) in enumerate(testLoader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            testLoss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    print(f'Test Loss: {testLoss/(batchIdx+1)} | Acc: {100.*correct/total}')\n",
        "\n",
        "\n",
        "for epoch in range(0, 200):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "id": "zPDMjAMYovq3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}